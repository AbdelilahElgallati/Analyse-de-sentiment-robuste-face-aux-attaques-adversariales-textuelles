{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEAimX4gvPy-",
        "outputId": "b3b4ce48-d0b3-486c-c47e-7961dae943f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy nltk scikit-learn contractions textblob joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BElMGt-ArOkH",
        "outputId": "4cc12720-3d86-426b-ec36-ecc9945bd326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import contractions\n",
        "from textblob import TextBlob\n",
        "import logging\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('omw-1.4')\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Configuration\n",
        "DATA_DIR = Path('/content/drive/MyDrive/Colab Notebooks/Data_RO')\n",
        "TWITTER_DATA_PATH = DATA_DIR / 'training.1600000.processed.noemoticon.csv'\n",
        "PROCESSED_DATA_PATH = DATA_DIR / 'processed_twitter_data.csv'\n",
        "RANDOM_STATE = 42\n",
        "TRAIN_TEST_SPLIT = 0.8\n",
        "VALIDATION_SPLIT = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uNI7vtUitNIl"
      },
      "outputs": [],
      "source": [
        "class TwitterDataPreprocessor:\n",
        "    \"\"\"\n",
        "    Comprehensive data preprocessing pipeline for Twitter sentiment analysis\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def load_twitter_data(self, file_path):\n",
        "        \"\"\"\n",
        "        Load Twitter dataset from CSV file\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the Twitter CSV file\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Loaded dataset\n",
        "        \"\"\"\n",
        "        logger.info(f\"Loading Twitter data from {file_path}\")\n",
        "\n",
        "        # Column names based on the dataset structure\n",
        "        column_names = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
        "\n",
        "        try:\n",
        "            # Load with Latin-1 encoding (common for this dataset)\n",
        "            df = pd.read_csv(file_path, encoding='latin-1', names=column_names)\n",
        "            logger.info(f\"Successfully loaded {len(df)} tweets\")\n",
        "\n",
        "            # Convert sentiment labels (0 -> 0, 4 -> 1)\n",
        "            df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})\n",
        "\n",
        "            # Keep only sentiment and text columns\n",
        "            df = df[['sentiment', 'text']].copy()\n",
        "\n",
        "            # Remove any missing values\n",
        "            df = df.dropna()\n",
        "\n",
        "            logger.info(f\"After cleaning: {len(df)} tweets\")\n",
        "            logger.info(f\"Sentiment distribution:\\n{df['sentiment'].value_counts()}\")\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading data: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"\n",
        "        Clean and preprocess a single text string\n",
        "\n",
        "        Args:\n",
        "            text (str): Raw text to clean\n",
        "\n",
        "        Returns:\n",
        "            str: Cleaned text\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Expand contractions (don't -> do not)\n",
        "        text = contractions.fix(text)\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Remove user mentions and hashtags\n",
        "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "\n",
        "        # Remove special characters and digits, keep only letters and spaces\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def tokenize_and_lemmatize(self, text):\n",
        "        \"\"\"\n",
        "        Tokenize text and apply lemmatization\n",
        "\n",
        "        Args:\n",
        "            text (str): Text to tokenize\n",
        "\n",
        "        Returns:\n",
        "            list: List of lemmatized tokens\n",
        "        \"\"\"\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords and lemmatize\n",
        "        tokens = [\n",
        "            self.lemmatizer.lemmatize(token)\n",
        "            for token in tokens\n",
        "            if token not in self.stop_words and len(token) > 2\n",
        "        ]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def preprocess_dataset(self, df, sample_size=None):\n",
        "        \"\"\"\n",
        "        Apply full preprocessing pipeline to the dataset\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input dataframe\n",
        "            sample_size (int): Optional sample size for faster processing\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Preprocessed dataset\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting dataset preprocessing...\")\n",
        "\n",
        "        # Sample data if requested (for faster experimentation)\n",
        "        if sample_size and sample_size < len(df):\n",
        "            df = df.sample(n=sample_size, random_state=RANDOM_STATE)\n",
        "            logger.info(f\"Sampled {sample_size} tweets for processing\")\n",
        "\n",
        "        # Clean text\n",
        "        logger.info(\"Cleaning text...\")\n",
        "        df['cleaned_text'] = df['text'].apply(self.clean_text)\n",
        "\n",
        "        # Remove empty texts\n",
        "        df = df[df['cleaned_text'].str.len() > 0]\n",
        "\n",
        "        # Tokenize and create processed text\n",
        "        logger.info(\"Tokenizing and lemmatizing...\")\n",
        "        df['tokens'] = df['cleaned_text'].apply(self.tokenize_and_lemmatize)\n",
        "        df['processed_text'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "        # Remove texts that are too short after processing\n",
        "        df = df[df['processed_text'].str.len() > 10]\n",
        "\n",
        "        # Add text length features\n",
        "        df['text_length'] = df['cleaned_text'].str.len()\n",
        "        df['word_count'] = df['processed_text'].str.split().str.len()\n",
        "\n",
        "        logger.info(f\"Preprocessing complete. Final dataset size: {len(df)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_train_test_split(self, df):\n",
        "        \"\"\"\n",
        "        Create train/validation/test splits\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Preprocessed dataset\n",
        "\n",
        "        Returns:\n",
        "            tuple: (X_train, X_val, X_test, y_train, y_val, y_test)\n",
        "        \"\"\"\n",
        "        logger.info(\"Creating train/test splits...\")\n",
        "\n",
        "        X = df['processed_text'].values\n",
        "        y = df['sentiment'].values\n",
        "\n",
        "        # First split: train+val vs test\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "            X, y,\n",
        "            test_size=1-TRAIN_TEST_SPLIT,\n",
        "            random_state=RANDOM_STATE,\n",
        "            stratify=y\n",
        "        )\n",
        "\n",
        "        # Second split: train vs validation\n",
        "        val_size = VALIDATION_SPLIT / TRAIN_TEST_SPLIT\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_temp, y_temp,\n",
        "            test_size=val_size,\n",
        "            random_state=RANDOM_STATE,\n",
        "            stratify=y_temp\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Train set: {len(X_train)} samples\")\n",
        "        logger.info(f\"Validation set: {len(X_val)} samples\")\n",
        "        logger.info(f\"Test set: {len(X_test)} samples\")\n",
        "\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "    def get_text_statistics(self, df):\n",
        "        \"\"\"\n",
        "        Generate statistics about the text data\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Dataset to analyze\n",
        "\n",
        "        Returns:\n",
        "            dict: Statistics dictionary\n",
        "        \"\"\"\n",
        "        stats = {\n",
        "            'total_samples': len(df),\n",
        "            'avg_text_length': df['text_length'].mean(),\n",
        "            'avg_word_count': df['word_count'].mean(),\n",
        "            'sentiment_distribution': df['sentiment'].value_counts().to_dict(),\n",
        "            'max_text_length': df['text_length'].max(),\n",
        "            'min_text_length': df['text_length'].min()\n",
        "        }\n",
        "\n",
        "        return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnjR5IJ4tU0n",
        "outputId": "995f8036-2e68-41ff-e15f-d068f7a42cd5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Initialize preprocessor\n",
        "preprocessor = TwitterDataPreprocessor()\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Check if data file exists\n",
        "if not TWITTER_DATA_PATH.exists():\n",
        "    logger.error(f\"Data file not found: {TWITTER_DATA_PATH}\")\n",
        "    logger.info(\"Please upload training.1600000.processed.noemoticon.csv to your Google Drive in the twitter_data directory\")\n",
        "else:\n",
        "    # Load and preprocess data\n",
        "    df = preprocessor.load_twitter_data(TWITTER_DATA_PATH)\n",
        "\n",
        "    # For initial testing, use a smaller sample\n",
        "    # Comment out the sample_size parameter to process the full dataset\n",
        "    df_processed = preprocessor.preprocess_dataset(df, sample_size=50000)\n",
        "\n",
        "    # Generate statistics\n",
        "    stats = preprocessor.get_text_statistics(df_processed)\n",
        "    logger.info(f\"Dataset statistics: {stats}\")\n",
        "\n",
        "    # Create train/test splits\n",
        "    splits = preprocessor.create_train_test_split(df_processed)\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = splits\n",
        "\n",
        "    # Save processed data\n",
        "    processed_data = {\n",
        "        'X_train': X_train,\n",
        "        'X_val': X_val,\n",
        "        'X_test': X_test,\n",
        "        'y_train': y_train,\n",
        "        'y_val': y_val,\n",
        "        'y_test': y_test,\n",
        "        'stats': stats\n",
        "    }\n",
        "\n",
        "    joblib.dump(processed_data, DATA_DIR / 'processed_splits.pkl')\n",
        "\n",
        "    # Also save the full processed dataframe\n",
        "    df_processed.to_csv(PROCESSED_DATA_PATH, index=False)\n",
        "\n",
        "    logger.info(\"Data preprocessing completed successfully!\")\n",
        "    logger.info(f\"Processed data saved to {PROCESSED_DATA_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtipvJzixkEA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
